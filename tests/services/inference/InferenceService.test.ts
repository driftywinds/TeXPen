/* eslint-disable @typescript-eslint/no-explicit-any */
import { describe, it, expect, beforeEach, afterEach, vi, beforeAll } from 'vitest';
import { InferenceService } from '../../../services/inference/InferenceService';
import { InferenceOptions } from '../../../services/inference/types';

// Mock DownloadManager to prevent 300MB downloads while keeping Service logic "legit"
vi.mock('../../../services/downloader/DownloadManager', () => {
  return {
    downloadManager: {
      downloadFile: vi.fn().mockImplementation(async (url: string, onProgress) => {
        // Simulate "fast" Network/Cache behavior by writing dummy files to the cache
        // This ensures the Service thinks the file is available and proceeds to usage
        if (typeof caches !== 'undefined') {
          const cache = await caches.open('transformers-cache');

          let content: BodyInit = new Uint8Array([0x08, 0x00]); // Minimal binary-like content
          // Note: transformers.js validation might check magic bytes or protobuf validity.
          // Invalid protobuf will cause from_pretrained to throw, which we expect and assert on.

          let headers: any = { 'content-length': '2' };

          if (url.endsWith('.json')) {
            // Provide minimal valid JSON for tokenizer/config to pass basic parsing if possible
            const dummyConfig = {
              model_type: 'vision-encoder-decoder',
              encoder: { model_type: 'vit' },
              decoder: { model_type: 'gpt2' },
              is_encoder_decoder: true
            };
            content = JSON.stringify(dummyConfig);
            headers = { 'content-length': content.length.toString(), 'content-type': 'application/json' };
          }

          await cache.put(url, new Response(content, { headers }));
        }

        // Simulate progress for the "download"
        if (onProgress) onProgress({ loaded: 100, total: 100, file: url });
      })
    }
  };
});

describe('InferenceService Integration (Efficient)', () => {
  let inferenceService: InferenceService;

  beforeAll(() => {
    // Simulate WebGPU availability for the fallback test
    // We want to test that it TRIES WebGPU, then falls back if it fails (or if we force it)
    Object.defineProperty(navigator, 'gpu', {
      value: {
        requestAdapter: vi.fn().mockResolvedValue({
          requestDevice: vi.fn().mockResolvedValue({
            limits: { maxStorageBufferBindingSize: 2147483648 } // Big enough
          })
        })
      },
      configurable: true
    });
  });

  beforeEach(() => {
    // Reset singleton if needed or just get instance
    inferenceService = InferenceService.getInstance();
    vi.clearAllMocks();
  });

  afterEach(async () => {
    try {
      // Cleanup if needed
    } catch (_e) {
      // ignore
    }
  });

  it('should attempt initialization and download without hitting network (Legit Flow)', async () => {
    // Suppress console.error for expected failures
    const consoleSpy = vi.spyOn(console, 'error').mockImplementation(() => { });

    // We expect this to fail at the "Load Model" stage because we provided DUMMY ONNX files.
    // However, getting to that failure proves that:
    // 1. DownloadManager was called correctly (and "downloaded" our dummy).
    // 2. InferenceService proceeded to initialize transformers.
    // 3. from_pretrained attempted to parse the file.
    // This verifies the INTEGRATION without the 300MB cost.

    const options: InferenceOptions = {
      device: 'cpu' as any, // Use CPU in Node environment to avoid WASM/WebGPU errors
    };

    console.log('[Test] Starting legit initialization with mocked network...');

    try {
      await inferenceService.init((status) => {
        console.log(`[Test Progress] ${status}`);
      }, options);
    } catch (error: any) {
      console.log('[Test] Caught expected error during loading:', error.message);
      // We expect failure because the ONNX files are dummy/empty.
      // We accept:
      // 1. Protobuf/Parsing errors (ideal)
      // 2. "Unsupported device" (if it ignored our 'cpu' option for some reason)
      // 3. "failed to create session" (onnxruntime error)
      // 4. "Unable to get model file path or buffer" (transformers.js error for bad files)
      const isExpectedError = /protobuf|offset|invalid|wire type|illegal|buffer|onnx|unsupported device|session|Unable to get model file path or buffer/i.test(error.message);

      expect(isExpectedError).toBe(true);
    } finally {
      consoleSpy.mockRestore();
    }

    // specific check: DownloadManager must have been utilized
    const { downloadManager } = await import('../../../services/downloader/DownloadManager');
    expect(downloadManager.downloadFile).toHaveBeenCalled();
  }, 30000);

  it('should handle concurrent requests gracefully (simulated)', async () => {
    // Suppress console.error for this test as well
    const consoleSpy = vi.spyOn(console, 'error').mockImplementation(() => { });

    try {
      // We expect this to fail because providing empty blob and no model loaded.
      // But we want to ensure it doesn't crash the process or hang.
      // We'll also force it to fail fast by not having a model.

      // Inject a 'cpu' config to avoid WASM error if it tries to init
      // However, infer() calls init() with defaults.
      // We can manually call init with CPU first to set the state, 
      // but init() will fail (as seen in previous test).

      // So we just catch the error and verify it's the expected one.
      await inferenceService.infer(new Blob([]), {});
    } catch (e: any) {
      // Expected since init failed or model is missing
      expect(e).toBeDefined();
    } finally {
      consoleSpy.mockRestore();
    }
  });
});
